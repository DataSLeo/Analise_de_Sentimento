{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Sak7HmwWb6ut","ZPlX1EAYcFuf"],"authorship_tag":"ABX9TyMoj9mgJPh9OBauJWGgQJ7M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sistema de Análise de Sentimento NLTK"],"metadata":{"id":"LPRHcJW5buK_"}},{"cell_type":"markdown","source":["## 1. Importando pacotes"],"metadata":{"id":"Sak7HmwWb6ut"}},{"cell_type":"code","execution_count":47,"metadata":{"id":"xgrwoPVeIcVn","executionInfo":{"status":"ok","timestamp":1715273748481,"user_tz":180,"elapsed":263,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}}},"outputs":[],"source":["# Importações referentes ao NLTK\n","import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet"]},{"cell_type":"markdown","source":["### A classe SentimetIntensityAnalyzer foi otimizada para trabalhar com a língua inglesa, por isso este projeto irá usar o google translator"],"metadata":{"id":"FkwONdTXdKKN"}},{"cell_type":"code","source":["# Importações referentes ao google translator\n","\n","# !pip install googletrans==4.0.0-rc1\n","from googletrans import Translator"],"metadata":{"id":"_8KNKkfBMOLT","executionInfo":{"status":"ok","timestamp":1715273750272,"user_tz":180,"elapsed":273,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["## 2. Configurando o NLTK"],"metadata":{"id":"ZPlX1EAYcFuf"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('words')\n","nltk.download('vader_lexicon')\n","nltk.download('wordnet')"],"metadata":{"id":"MWeizNIAMf3K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Construindo a classe Sentiment"],"metadata":{"id":"HbbD4R9XcTTV"}},{"cell_type":"markdown","source":["### A classe criada chamada Sentiment tem a função de faciliar os tratamentos necessários para a obtenção dos sentimentos das frases, tendo as funções:\n","\n","1. import_text [Inserir os textos na classe]\n","2. get_text [Obter os textos]\n","3. to_english [Traduzir as frases português para inglês]\n","4. tokenize [Tokenizador]\n","5. lemma [Lematizador]\n","6. sentiment [Obter de fato os sentimentos]\n","\n","No final de tudo a Classe Sentiment irá retornar um dicionário de: positive, negative e neutral."],"metadata":{"id":"tdYTczrhdyEH"}},{"cell_type":"code","source":["class Sentiment:\n","  def __init__ (self):\n","    self.text = ''\n","\n","  # Um processo que trabalha junto com o lemma\n","  # Este processo tem a função de converter o pos_tag para algo que o lemmatizer entende\n","  def __LEMMA__(self, treebank_tag):\n","    if treebank_tag.startswith('J'):\n","      return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","      return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","      return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","      return wordnet.ADV\n","    else:\n","      return wordnet.NOUN\n","\n","\n","  # Entrada do texto\n","  def import_text(self, text):\n","    self.text = text\n","\n","\n","  # Obter o texto\n","  def get_text(self):\n","    return self.text\n","\n","\n","  # Tranformar o texto português em inglês\n","  def to_english(self, force=False):\n","    text_ = []\n","    translator = Translator()\n","    token = self.tokenize(\"sent\")\n","\n","    for t in token:\n","      for x in t:\n","        en = translator.translate(x, src='pt', dest='en').text\n","        text_.append(en)\n","\n","    original = self.text\n","\n","    if force:\n","      self.text = text_\n","\n","    return {\"original\": original, \"english\": text_}\n","\n","\n","  # Tokenizando o texto\n","  # Este processo é mais usado dentro de outras funções\n","  # Se for separar em palavras use o _type = \"word\"\n","  # Se for separar em sentenças use o _type = \"sent\"\n","  def tokenize(self, _type=\"word\"):\n","    text = []\n","\n","    if _type == \"word\":\n","      for t in self.text:\n","        w = nltk.word_tokenize(t, language=\"portuguese\")\n","        text.append(w)\n","\n","    elif _type == \"sent\":\n","      for t in self.text:\n","        w = nltk.sent_tokenize(t, language=\"portuguese\")\n","        text.append(w)\n","\n","    return text\n","\n","\n","  # Transforma as palavras em sua forma básica\n","  def lemma(self, force=False):\n","    text = []\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for lem in self.text:\n","      w = [word for word in nltk.word_tokenize(lem)]\n","      tagged = [tag for tag in nltk.pos_tag(w)]\n","\n","      lemmatized_sentence = ' '.join([lemmatizer.lemmatize(word, self.__LEMMA__(tag)) for word, tag in tagged])\n","\n","      text.append(lemmatized_sentence)\n","\n","    original = self.text\n","\n","    if force:\n","      self.text = text\n","\n","    return {\"original\": original, \"lemmatized\": text}\n","\n","\n","  # Transforma o texto em um dicionário de positive, negative e neutral\n","  def sentiment(self):\n","    text = []\n","    sia = SentimentIntensityAnalyzer()\n","    score = {\n","        'positive': {\n","            'total': 0,\n","            'phrases': []\n","        },\n","        'negative': {\n","            'total': 0,\n","            'phrases': []\n","        },\n","        'neutral': {\n","            'total': 0,\n","            'phrases': []\n","        },\n","        'total': 0\n","    }\n","\n","    for t in self.text:\n","\n","      s = sia.polarity_scores(t)\n","\n","      if s['compound'] > 0.4:\n","        score['positive']['total'] += 1\n","        score['positive']['phrases'].append(t)\n","      elif s['compound'] < -0.4:\n","        score['negative']['total'] += 1\n","        score['negative']['phrases'].append(t)\n","      else:\n","        score['neutral']['total'] += 1\n","        score['neutral']['phrases'].append(t)\n","\n","      score['total'] += 1\n","\n","    return score"],"metadata":{"id":"F3fy119pNgNP","executionInfo":{"status":"ok","timestamp":1715278061915,"user_tz":180,"elapsed":275,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}}},"execution_count":203,"outputs":[]},{"cell_type":"markdown","source":["## 4. Usando a classe para o sentimento"],"metadata":{"id":"ppLbv3Vrcflz"}},{"cell_type":"code","source":["s = Sentiment()"],"metadata":{"id":"2xc3CaxqMqYR","executionInfo":{"status":"ok","timestamp":1715278063614,"user_tz":180,"elapsed":499,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":["s.import_text([\n","    'Gostei deste produto, achei ele demais',\n","    'Não gostei muito deste produto',\n","    'Achei interessante este produto',\n","    'Gostei muito do produto XYZ',\n","    'Pior produto que já vi'\n","])"],"metadata":{"id":"APkJP6glfOE_","executionInfo":{"status":"ok","timestamp":1715278064043,"user_tz":180,"elapsed":3,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}}},"execution_count":205,"outputs":[]},{"cell_type":"code","source":["trad = s.to_english(force=True)\n","\n","# texto em português\n","print(\"Português\")\n","\n","for i, x in enumerate(trad['original']):\n","  print(i+1, \". \", x)\n","\n","\n","# texto em inglês\n","print(\"\\nInglês\")\n","\n","for i, x in enumerate(trad['english']):\n","   print(i+1, \". \", x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E37A8I0fQk9","executionInfo":{"status":"ok","timestamp":1715278065994,"user_tz":180,"elapsed":530,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}},"outputId":"51d300ff-669c-4f79-a7dd-fc4ff5543d55"},"execution_count":206,"outputs":[{"output_type":"stream","name":"stdout","text":["Português\n","1 .  Gostei deste produto, achei ele demais\n","2 .  Não gostei muito deste produto\n","3 .  Achei interessante este produto\n","4 .  Gostei muito do produto XYZ\n","5 .  Pior produto que já vi\n","\n","Inglês\n","1 .  I liked this product, I found it too much\n","2 .  I didn't like this product very much\n","3 .  I found this product interesting\n","4 .  I really liked the product xyz\n","5 .  Worst product I've ever seen\n"]}]},{"cell_type":"code","source":["posts_sent = s.sentiment()\n","\n","print(posts_sent['positive'])\n","print(posts_sent['negative'])\n","print(posts_sent['neutral'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qSucY8XQfT4P","executionInfo":{"status":"ok","timestamp":1715278068565,"user_tz":180,"elapsed":255,"user":{"displayName":"Leonardo Pereira","userId":"16198268585977869802"}},"outputId":"f0c0ae18-6eea-4846-cdce-6f578b0efcbe"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stdout","text":["{'total': 3, 'phrases': ['I liked this product, I found it too much', 'I found this product interesting', 'I really liked the product xyz']}\n","{'total': 1, 'phrases': [\"Worst product I've ever seen\"]}\n","{'total': 1, 'phrases': [\"I didn't like this product very much\"]}\n"]}]}]}